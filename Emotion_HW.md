# 목차
**i) 수치예측**
- **(1). 선형 회귀**
- **(2). 손실 함수&경사 하강법**

**ii) 이진 분류** 
- **(1). 퍼셉트론**
- **(2). 로지스틱 회귀**
- **(3). 시그모이드 함수(Sigmoid Function)**




**i) 수치예측**

**i)-1. 선형 회귀**

- **선형 회귀의 의미?**: **종속 변수 y**와 1개 이상의 **독립 변수 x**의 **관계를 가장 잘 나타내는 1차함수**를 찾는 것

- **인공지능에서의 사용?**: 데이터들 사이의 **대략적인 경향성**을 파악하기 위해 쓴다.

**i)-2. 손실 함수&경사 하강법**
 
- **손실 함수의 의미?**: 학습을 통해 얻은 **데이터의 추정치**가 **실제 데이터와 얼마나 차이**나는지에 대한 지표이다.

- **경사 하강법의 의미?**: **임의의 w값**에서 시작해서 **미분할 때 0인 지점**까지 **경사를 하강**시켜가며 **최소값을 찾는** 방법

- **인공지능에서의 사용?**: 선형 회귀를 통해 얻어진 직선의 **가장 최적화된 가중치**와 **편향**을 **찾기** 위해 사용한다.

**ii) 이진 분류** 

**ii)-1. 퍼셉트론**
 
- **퍼셉트론의 의미?**: **인공신경망 모형의 한 종류**이며 **여러 개의 입력값**을 받아 **하나의 출력값**을 **도출**한다.

- **인공지능에서의 사용?**: **신경망의 기초**가 된다. 다중 인공신경망을 학습하는 알고리즘이 바로 딥 러닝이다.

**ii)-2. 로지스틱 회귀**
 
- **로지스틱 회귀의 의미?**: **종속변수가 0과 1인 데이터**에 **선형회귀를 적용했을 때의 문제점을 해결**한 모델이다.

**인공지능에서의 사용?**: 인공지능이 **특정 데이터를 두 가지로 분류**를 할 때 필요하다.

**ii)-3. 시그모이드 함수(sigmoid function)**
 
- **시그모이드 함수의 의미?**: **s(z)=1/(1+e^-z)로 정의되는 함수**. **정의역**은 **모든 실수**, **치역**은 **0~1 사이의 실수**이다.
- **인공지능에서의 사용?**: **퍼셉트론** 등에서 **활성화 함수**(반환 값을 결정하는 함수)로 **사용**된다.
